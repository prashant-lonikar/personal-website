export const metadata = {
  title: 'Liquid AI',
  date: '2024-10-09',
  image: '/images/liquid-ai-banner.jpeg'
};

export const content = `
  # Liquid Foundational Models (LFMs)

*Posted by Prashant Lonikar on October 9, 2024*

Liquid Foundation Models (LFMs) were recently introduced by [Liquid AI](https://www.linkedin.com/company/liquid-ai-inc/), a startup spun off from MIT. Founded by researchers like [Ramin Hasani](https://www.linkedin.com/in/raminhasani/) and [Mathias Lechner](https://www.linkedin.com/in/mlech26l?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACUm3UAB4ESAt0kouYC0kfuatQ0C70xO2pI), they specialize in developing AI systems that differ from popular transformer-based architectures.

LFMs build on earlier work with liquid neural networks, inspired by brain-like dynamic systems that can adapt over time. Confused? Let’s break it down.

## Why are they called "liquid"?

These models are called "liquid" because their *model architecture* can adapt and adjust when given new information, much like how a liquid changes shape depending on its container. Unlike traditional models like GPT or Gemini that use transformer architecture, LFMs remain flexible even after training, allowing them to handle new information better.

## What is "model architecture"?

A *model’s architecture* is the design or structure of how it processes data. Just like a building's architecture determines its shape and purpose, a model’s architecture guides what kinds of tasks it can handle and how efficiently it does so.

## ...and "transformers"?

*Transformers* are a popular type of model architecture, used in successful models like ChatGPT, which excel at handling large amounts of data and finding patterns. However, transformers require a lot of memory and computing power.

## Why use liquid models instead of transformers?

Liquid models don't use transformers. Instead, they rely on mathematical and systems theory to be more *memory-efficient*, which means they use less memory while processing long pieces of data. This makes them ideal for devices with less memory, such as mobile phones or laptops.

## What does "memory-efficient" mean?

A *memory-efficient* model can do its job without needing a lot of computer memory. This is important because it allows models to run on smaller devices and use less power. Liquid models are designed to handle large amounts of data without the heavy memory usage seen in models like ChatGPT.

## Practical improvements using liquid models

Liquid models can perform tasks requiring large amounts of data (like analyzing long documents or videos) on devices with limited memory or computing power—known as *edge devices*. This efficiency means powerful AI can be used for real-time applications like chatbots or document processing without needing expensive hardware.

For everyday users, liquid models might not feel drastically different from ChatGPT for generating text or looking up information. But for more complex tasks—like understanding longer conversations or reading lengthy documents—liquid models will make AI feel faster and more responsive.

## Public reaction: Paradigm shift or minor improvement?

Public reaction has been mostly positive, with many seeing LFMs as a potential breakthrough in AI. Their key innovation—*memory efficiency*—allows them to process longer sequences of data without the heavy memory demands of transformer models. For example, the [LFM-3B model](https://siliconangle.com/2024/09/30/liquid-ai-debuts-new-lfm-based-models-seem-outperform-traditional-llms/) can handle tasks like analyzing long documents while using less memory than models like Microsoft’s Phi-3.5 or Meta’s Llama.

While LFMs are impressive on paper and set new standards in their parameter categories (such as LFM-1B), experts remain cautious. The real test will be in how they perform in broader real-world applications.

## Want to try it out?

You can! Visit the [Liquid Playground](https://playground.liquid.ai/chat?model=cm1ooqdqo000208jx67z86ftk) and test your usual prompts across various models. While multimodal features aren’t available yet, keep an eye out for future updates!

  `;